#+TITLE: 005 Configuration Management And The Cloud

* Week 1: Automating with Configuration Management
** Course Introduction
*** Course Introduction
In this course we will learn:
- How to apply automation to manage fleets of computers
- How to automate deploying new computes
- How to keep machines updated
- How to manage large-scale changes
- And more.

Both for physical machines and virtual machines on the cloud.

What is SRE?
    Site Reliability Engineering is focused in the reliability and maintainability of large systems. In the process, apply a lot of automation to manage them.

Configuration management: lets us manage the configuration of our computers at scale.
** Introduction to Automation at Scale
*** Introduction to Module 1: Automating with Configuration Management
*** What is Scale?
Being able to *scale* what we do means that we can keep achieving larger impacts with the same amount of effort. In short, a scalable service is a flexible one.

Adding more computers can be easy or really hard, depending on how the infrastructure is set up.

Automation is an essential tool for keeping up with the infrastructure needs of a growing business.
*** What is Configuration Management?
E.g. imagine you want to add a new server, manually setting it up is called unmanaged configuration (OS, applications, configuration files, policies, etc... ).

Managed configuration means using a configuration management system to manage all the computers in the fleet, also known as nodes. Typically, you define a set of rules that have to be applied to the nodes you want to manage and then have a process that ensures that those settings are true on each of the nodes.

At a small scale, unmanaged configuration seems inexpensive. However, adding new services can take a lot of time and when things go wrong (which often happens), it can take a lot of time to get back online.

Using a configuration management, the changes you make to a system or a group of systems are done in a systematic, repeatable way. It also makes making changes efficient and consistent.

Configuration management system can handle with some kind of errors by themselves. E.g. if an user changes a configuration file, the configuration management will let you know and will also restore the file to the state before the user changed it.

Examples of configuration managers: Puppet, Chef, Ansible, CFEngine

    These tools can be used to manage locally hosted infrastructure (bare metal such as laptops and virtual machines). Some also are capable of cloud integration (such as AWS EC2, Microsoft Azure, or Google Cloud platform).

    There are also some platform specific tools like SCCM and Group Policy for Windows.

To make the most out of our configuration management system, use the infrastructure as code paradigm.
*** What is infrastructure as code?
When we used a configuration management system, we write rule that describe how the computers in our fleet should be configured. These rules are then executed by the automation, to make the computers match our desired state. This means that we can model the behavior of our IT infrastructure in files that can be processed by automation tools. These files can be tracked in a version control system which is super useful when we need to revert changes.

The paradigm of storing all the configuration for the managed devices in version controlled files is known as infrastructure as code or IaC.

    Infrastructure as code (IaC): When all the configuration necessary to deploy and manage a node in the infrastructure is stored in version control.

The principles of IaC are commonly applied to cloud computing environments where machines are treated like interchangeable resources, instead of individual computers.

Even if there is only one computer, using VCS to track configuration files has advantages such are having the file reviewed by others, rolling back, and quickly set up a new computer if the one in use fails as well as setting up automated tests.

Managing your infrastructure as code means that your fleet of nodes are consistent, versioned, reliable, and repeatable.
** Introduction to Puppet
*** What is Puppet?
Puppet is the current industry standard. It is popular open-source project which is cross platform.

Puppet is usually deployed in a client-server architecture. The client is known as the Puppet Agent and the server is known  as the Puppet Master. The agent sends a list of 'facts' to the Master and the Master then sends a set of 'Rules' that need to be applied to the device.

Example of a Rule:

    class sudo {
        package { 'sudo':
            ensure => present,
            }
    }

This block is saying that the package named 'sudo' has to be present wherever the rule is applied, an if it is not found, then install it.

Puppet will determine the type of OS and use the right tool to install the package. However, for windows, we'll need to add an extra attribute to our rule stating where the installer file is located on the local disk or a network mounted resource or add a an extra 'Chocolately' provider to Puppet.

Puppet can also add, remove, modify configuration files stored on the system  or change registry entries on windows. We can also enable, disable, start, or stop the services that run on our computers.

We can also configure crone jobs or schedule tasks, add, remove, or modify users and groups or even execute external commands.
*** Puppet Resources
Resources: The basic unit for modeling the configuration that we want to manage.

    In other words, each resource specifies one configuration that we are trying to manage (like a service, package, or a file)

File Resource Example 1:

    class sysctl {
        # Make sure the dictionary exists, some distros do not have it
        file { '/etc/sysctl.d':
            ensure => directory
        }
    }

This block ensures that the directory exists.

File Resource Example 2:

    class timezone {
    file { '/etc/timezone':
        ensure => file,
        content => "UTC\n",
        replace => true,
        }
    }

This block configures the file 'timezone'. Here, the resource configuration states the 'timezone' needs to be a file, set the contents of the file to UTC time zone, and set 'replace' to true meaning that if there is an existing file with that name, it will replace the file.

You can also change permissions, file owners, or modification time.

When declaring rules, we are stating the desired state of the resource in the system. The puppet Agent turns this desired state into reality using providers.

The provider used will depend on the resource defined and the environment where the agent is running. Puppet will normally detect this automatically.
*** Puppet Classes
Classes are used to collect the resources that are needed to achieve a goal in a single place. E.g. you could have a class that installs a packages, modifies its configuration file, and starts the service provided by that package.

    E.g. a class with three resources, all related to 'NTP' (Network Time Protocol), the mechanism our computers use to synchronize the clocks.

        class ntp {
            package { 'ntp':
                ensure => latest,
            }
            file { '/etc/ntp.conf':
                source => 'puppet://modules/ntp/ntp.conf'
                replace => true,
                }
            service {'ntp':
                enable => true,
                ensure => running,
                }
        }

    The class ensures that the package is always updated to the latest version, the contents of the configuration file is specified and set to replace any existing file, and to ensure that the service is running.

    By grouping related resources into a single class, we get the advantages of ensuring efficiency and convenience for future changes.
*** puppet docs
https://puppet.com/docs/puppet/7/lang_resources.html
** The Building Blocks of Configuration Management
*** What are domain-specific languages? DSL
java, python, go are general purposes languages.

Domain specific language: A programming language that's more limited in scope.

The DSL in puppet is limited to operations related to when and how to apply configuration management rules. Puppet's DSL includes variables, conditional statements, and functions.

Puppet's facts: variables that represent the characteristics of the system.

    There are many built-in facts into puppet such as facts to store the node OS, IP address, memory available, etc...

    E.g. Facts usage ('smartmontools is a package used for monitoring the state of hard drives using 'smart' command' which does not make sense to have in virtual machines)

        if $facts['is_virtual'] {
            package {'smartmontools':
                ensure => purged,
            }
        } else {
            package{'smartmontools':
                ensure => installed,
            }
        }


        The '$facts' variable is a puppet's DSL hash (which is equivalent to a dictionary in python).
*** The Driving Principles of Configuration Management

Puppet uses as declarative language, because we declare the state that we want to achieve rather than the steps to get there.

Python and C are procedural languages because we write out the procedure that the computer needs to follow to reach our desired goal.

Operations in configuration management should be idempotent. An *idempotent* action can be performed over and over again without changing the system after the first time action was performed, and with no unintended side effects.

    If a script is idempotent, it means it can fail halfway through its task and be run again without problematic consequences.

        Most puppet resources provide idempotent actions.

            A notable exception is the 'exec' resource which runs commands for us and these commands can modify the system each time it is executed. E.g. moving a file, this will only work once and yield a puppet error otherwise. There is a workaround:

                exec { 'move example file':
                    command => 'mv /home/user/example.txt /home/user/Desktop',
                    onlyif => 'test -e /home/user/example.txt'
                }

            This is now a idempotent action because it will only move the file if 'onlyif' is true (i.e. test that the file we are trying to move exists in the directory).


Test and repair paradigm: Meaning that actions are taken only when they are necessary to achieve a goal. For example, only install a package if it hasn't been installed before.

Poppet is stateless, each puppet run is independent of each previous puppet run. Puppet does not save the state between runs of the agents.
*** More Information About Configuration Management
http://radar.oreilly.com/2015/04/the-puppet-design-philosophy.html
*** Assignment qwiklabs
/etc/profile.d/ is a directory used to store scripts which will perform startup tasks, including setting up a user's own environment variables.

PATH variable: environment variable that contains an ordered list of paths that Linux will search for executables when running a command. This is useful because it means we do not have to specify the absolute path for each command we want to run.

    The PATH variable typically contains a few different paths which are separated bY colons.

**** tip
To append a path directory to the PATH environment variable using puppet, you can use

class profile {
    file { '/etc/profile.d/append-path.sh':
        owner => 'root',
        group => 'root',
        mode => '0644',
        content => "PATH=\$PATH:/java/bin\n"
    }
}

To trigger a manual run of the puppet agent by running:
'$ sudo puppet agent -v --test'
* Week 2: Deploying Puppet
** Deploying Puppet Locally
*** Intro to Module 2: Deploying Puppet
We will install puppet locally and set up a test set-up. Also, we'll see how to set up a client-server set-up with puppet clients connecting and authenticating to the Puppet server.
*** Applying rules locally
Puppet is usually deployed in a client-server architecture but that is not the only way we can use puppet. We can use it as a stand alone command line application. This can be the preferred configuration for complex setups where connecting to a master is no longer the best approach.
**** to install puppet
just intall puppet using the OS package manager such as pacman or apt. E.g. '$ sudo pacman -S puppet'
**** Example usage, debugging tools
You can create the most simple file, in this case you want to use ensure that some debugging tools are available at all the nodes. The file that stores this configuration (rules) is called manifest and has extension .pp, for example 'tools.pp'

    Example 'tools.pp':

        package { 'htop':
            ensure => present,
        }

    This file installs htop if it is not installed already. To apply the rules, run:
    '$ sudo puppet apply -v tools.pp'
        The '-v' flag runs puppet in verbose mode so that shows text messages to the terminal.
**** Puppet's catalog
After loading all facts for a computer, the server calculates which rules actually need to be applied. For example, if a packet should only be installed when a certain condition is met, this condition is evaluated on the server side based on the gathered facts.

The catalog is the list of rules that are generated for one specific computer once the server has evaluated all variables, conditionals, and functions.
*** Managing Resource Relationships
Puppet's manifests usually include a bunch of resources that are related to each other. You cannot start a service without the configuration, and you cannot change the configuration without the package installed. Puppet manages this resource relationships.

Example:

    class ntp {
        package { 'ntp':
            ensure => latest,
        }
        file { '/etc/ntp.conf':
            source => '/home/user/ntp.conf',
            replace => true,
            require => Package['ntp'],
            notify => Service['ntp'],
        }
        service{ 'ntp':
            enable => true,
            ensure => running,
            require => File['/etc/ntp.conf'],
        }
    }
    include ntp

In this example, resource types are written in lowercase whereas relationships are written with an uppercase for the first letter of the resource.

We write resource types in lowercase when declaring them, but capitalize them when referring to them from another resource's attribute.

In this example, there is also a call to include 'ntp' at the end of the file. Usually, the class is defined in one file and include it in another one.
*** Organizing Your Puppet Modules
Module: A collection of manifests and associated data.

You want to organzine related the manifests under a sensible topic name. E.g you can have a module dedicated to monitor the computer's health, another for setting up the network stack, and yet another one for configuring a web serving application.

All manifest get stored in a directory called manifests, the rest of the data is stored in different directories depending on what it does. E.g. files such as 'ntp.conf' are stored in the 'files' directory. Another example is the 'templates' directory which stores files that are preprocessed before being copied to the client machines. These templates can include values that get replaced after calculating the manifests, or sections that are only present if certain conditions are valid.
**** Summary
So basically, a module is a directory (within the modules directory) which has a manifest, files, templates, etc directories. In it, there should be a 'init.pp' file in the 'manifest' directory and it should define a class with the same name as the module you are creating. 'init.pp' needs to be present since it is the first file that puppet reads when a module gets included.

Any files that your rules use should be stored in the 'files' directory or if they need to be preprocessed, in the 'templates' directory.

These modules can be shared. They can be installed through the OS package manager like this: (in this case, ubuntu) '$ sudo apt install puppet-module-puppetlabs-apache'

    To include a module from a manifest, you can create a file such as 'webserver.pp' like this:

        include ::apache

    The two columns before the 'apache' lets puppet know that this is a global module named 'apache'.

    Finally, to apply the manifest, use the same command as last time:
        '$ sudo puppet apply -v webserver.pp'

**** tip
use '$ tree modules/' to print the structure of the directory 'modules'
*** More Information About Deploying Puppet Locally
The Puppet language style guide: https://puppet.com/docs/puppet/7/style_guide.html
Installing Puppet Server: https://puppet.com/docs/puppet/7/server/install_from_packages.html
** Deploying Puppet to Clients
*** Puppet Nodes
When managing a lot of computers, you might want to have certain rules apply to all of the computers, and some other rules just to a subset of those computers.

In puppet terminology, a node is any system where we can run a puppet agent (e.g. a physical work station, a server, a virtual machine, or even a network router).

For example, a rule definition for all the nodes:

    node default {
        class { 'sudo': }
        class { 'ntp':
                servers => ['ntp1.example.com', 'ntp2.example.com']
        }
    }

On the other hand, if you want some rules to apply to only certain nodes you can use something like this:

    node webserver.example.com {
        class {'sudo': }
        class { 'ntp':
                servers => ['ntp1.example.com', 'ntp2.example.com']
        }
        class { 'apache': }
    }

In this case, the node is identified using a FQDNs (Fully Qualified Domain Name) which is 'webserver.example.com'.

When a node requests which rules it should apply, puppet will look at the node definitions, figure out which one matches the node's FQDN and then give only those rules.

*Node definition usually happens in the 'site.pp' file*
*** Puppet's Certificate Infrastructure
puppets internal workflow:
1. puppet agent sends 'facts' to the puppet master.
2. The puppet master (server) then processes the manifests, generates the corresponding catalog (rules to be applied to that node).
3. The puppet master then sends it back to the puppet agent (client).
4. The puppet agent applies the changes locally.

How does puppet verify the nodes?
puppet uses public ket infrastructure, or PKI, to establish a secure connection between the server and the clients. There are a bunch of different PKI technologies but puppet uses 'secure socket layer or SSL' (same technology used for encrypting transmissions over HTTPS).

    Basically, all machines have two keys related to each other: private key and public key. The private key is secret, only known by that machine. The public key is shared with other machines involved. The sender uses its private key to sign messages and the receivers use the public key to validate the message.

    How do machines know which keys to trust?

        Certificate Authority (CA) verifies the identity of the machine and then creates a certificate stating that the public key goes with that machine.

        Puppet comes with its own CA but you can also use another CA.

When a puppet agent connects with the puppet master for the first time, the puppet master asks for the certificate (made by the CA) and if it can verify the node's identity, it creates a certificate for that node. This way, when a node picks up this certificate, it knows it can trust that puppet master and that the node can use the certificate to identify itself when requesting a catalog.

Why care about the security?
Puppet rules can hold sensitive information that you do not want it to get into the wrong hands. But also to makes sure that the webserver is actually the webserver and not just a rouge machine with the same name. If you are using puppet for managing tests machines, you can ask puppet to automatically sign all request but you should never do this on real computers with real users.

*Always authenticate your machines*

You can verify the identities of the machines automatically through a script. One way to do this is by copying a unique piece of information into the machines when they get provisioned and then use this pre-shared data as part of the certificate request.
*** Setting up Puppet Clients and Servers
For automatically signing puppet certificate request you can use this:
'$ sudo puppet config --section master set autosign true'
*but only use this when testing, never do this for real computers.* For real computers, you have to manually sign the requests or implement a proper validating script.

On the machine that is going to be a puppet agent node:
1. install puppet
2. run '$ sudo puppet config set server ubuntu.example.com' to set the server.
3. test using '$ sudo puppet agent -v --test'. This runs a test.
   This should create a SSL key, create a certificate request after reading a bunch of info, then shows us the fingerprint of the certificate request.
    This certificate request could be used to verify that the request and the server matches the one generated on the machine.
4. The certificate is generated on the puppet master.
5. The puppet agent stores the certificate locally.
6. The puppet agent retrieves the catalog and applies it.

You need to create a file on the 'production environment' like this: '/etc/puppet/code/environments/production/manifests/site.pp':

    node webserver.exmaple.com {
        class {'apache':}
    }
    node default {}

This is a node definition for the webserver and the default nodes. You can then apply this manifest at the webserver like this:
    From the puppet agent '$ sudo puppet agent -v --test'
        This should install apache and other services.

To keep puppet running constantly on the puppet agent, use systemctl.
    '$ sudo systemctl enable puppet'  This will start the puppet service whenever the computer starts up.
    '$ sudo systemctl start puppet' This will start the service now (similar to 'systemctl enable --now puppet')
*** More Information about Deploying Puppet to Clients
Info about SSL on puppet: https://www.masterzen.fr/2010/11/14/puppet-ssl-explained/
** Updating Deployments
*** Modifying and Testing Manifests
When changing a 'manifest', puppet applies these changes to the corresponding nodes.

Use the 'puppet parser validate' command that checks that the syntax of the manifest is correct.

Apply the changes using the '--noop' flag (NO OPerations) which simulates what puppet would do without actually doing it. This way, you can take a look at the list of actions that it would take and check that they're exactly what you wanted puppet to do. Another approach is to have a tests machines that are used exclusively for testing out changes. However, this is manual work and prone to mistakes.

You can test the manifest automatically using 'r-spec'. In these tests, we can set the 'facts' involved to the values that we want and check that the catalog comes out the way we wanted.

Here is a test example:

    describe 'gksu', :type => :class do
        let (:facts) { { 'is_virtual' => 'false' } }
        it {should contain_package('gksu').with_ensure('latest')}
    end


We can write a lot of these tests and run them all whenever there is a change to the rules. This ensures that the rules stay valid and that the new changes didn't break the old rules.

To check that the rules have the effect that we want, we can use a set of test machines where we first apply the catalog and then use scripts to check that the machines are behaving correctly.
*** Safely Rolling Out Changes and Validating Them
Even if the test machine works fine, it does not mean that the changes will work correctly on the machines running on production.

Production: The parts of the infrastructure where the service is executed and served to its users.

To roll out changes safely, first run them through a *test environment*. This environment should be one or two machines with the exact same configuration as the production environment.

Puppet allows for environment configurations. This way, you can completely isolate the configuration of a machine that the agent sees. This is not just for the modules on each node but also for the version of the module (e.g. test a newer version of the apache module). You can have a development environment (for IT specialists), a test environment for a specific feature, production environment, etc...

Pushing changes to the production environment at the same time is usually not a good idea. There is almost always a special case that we do not consider and has
the potential to take the machines offline. It is a better idea to roll out the change in small batches.

    You could have machines marked as early adopters or 'canaries'. These canaries are used to detect possible issues before they reach the other computers. If there is a problem, the damage is contain to only a subset of users.

Do not apply six months worth of changes at once. This will make it difficult to debug. Apply changes every one or two weeks.

*** More Information About Updating Deployments
Write puppet tests: https://rspec-puppet.com/tutorial/
Check that your puppet manifest conform to the style guide: http://puppet-lint.com/
** Module Review
Use version control systems to track the changes in the 'infrastructure as code' files.
* Week 3: Automation in the Cloud
** Cloud Computing
*** Intro to Module 3: Automation in the Cloud
*** Cloud Services Overview
A service is running in the cloud if the service is running somewhere else, either a data center or in other remote servers that we can reach over the internet.

Cloud providers offer a bunch of services. Among them, one of the most used services is Software-as-a-Service.

Software-as-a-Service (SaaS): When a Cloud provider delivers an entire application or program to the customer.

Platform-as-a-Service: (PaaS): When a Cloud provider offers a preconfigured platform to the customer.

Infrastructure-as-a-Service (IaaS): When a Cloud provider supplies only the bare-bones computing experience. (basically a VM with internet) (Amazon's EC2, Google Compute Engine, Microsoft Azure Compute)

When using Cloud resources, you need to consider regions. A region is a geographical location containing a number of data centers. Regions contain zones, and zones can have one or more data centers. You want to use a zone close to your users.
*** Scaling in the Cloud
It takes a lot of time to scale up or down on premises servers.

Capacity: How much the service can deliver.

Capacity is tied to the number and size of servers involved. Our capacity needs to change over time.


To scale _horizontally_, we add more nodes into the pool that's part of a specific service. Add more servers to increase your capacity.

To scale _vertically_, we make the nodes bigger. For example, change the 10GB memory to 100GB memory, upgrade the CPU and/or GPU.

Automatic scaling: The service offered by the Cloud provider will use metrics to automatically increase or decrease the capacity of the system.

Manual scaling: Changes are controlled by humans instead of software.

When using manual scaling, a sudden increase in demand can cause slow performance to a full outage.
*** Evaluating the Cloud
On premises infrastructure, you have the control of everything. On the other hand, while using Cloud services, you give up some of that control to the cloud provider.

When choosing SaaS, we are basically giving the provider complete control. We get a limited amount of settings. But we do not have to worry about making the system work. However, there is a limited amount of applications offered in such a prepackaged way. Ideal if you just want to focus on the software.

When choosing PaaS, we are in charge of the code but not in charge of running the application.

When choosing IaaS, we get to keep a high level of control. We get to choose the OS and applications installed on the virtual machines. We still get limited network configuration or limited services availability. If something does break, you might need to get support from the vendor to fix the problem.

Trade off between (good) not maintaining the physical machines and (bad) loosing control over hardware, network, and overall infrastructure.

Another bad aspect of cloud services is that you do not know what type of security measuers are being put in place. Make sure to check how they  secure the instances and your data. There are a bunch of certificates out there such as SOC 1, ISO 27001, etc...

Even if the cloud providers take security measures, that does not mean that that's it. Cloud users are still responsible for things such as credentials and firewalls. Other things worth mentioning are multi-factor authentication, encrypted file systems, or public key cryptography.
*** Migrating to the Cloud
When using IaaS, we deploy the services using virtual machines running on the Cloud Provider's infrastructure.

IaaS is useful for administrators since you can use 'lift-and-shift'. The term comes from physically lifting and shifting the servers to a new location. When migrating to the cloud, you migrate the server to a virtual machine running in the cloud.

If you are already using configuration managers to maintain the server, moving to the cloud can be pretty easy. You just apply the same config to the VMs on the cloud (IaaS).

Using PaaS is a great alternative when you have a specific infrastructure requirement but you do not want to get involved in the specifics of the platform. E.g. an SQL database: you do not have to worry about having the right disks attached, configuring the database, or any other task related to the machine set-up, just focus on using the database. Another example is a web app: you don't need to care about the framework. This can speed development since developers do not need to manage the platform and can focus on writing code.

PaaS examples include Amazon Elastic Beanstalk, Microsoft App Service, and Google App Engine. Although they are similar, migrating between platforms require some code changes.

Containers: Applications that are packed together with their configuration and dependencies. This allows the applications to run in the same way no matter the environment used to run them. In other words, you can run the app on the on-premises server, a cloud provider, or another cloud provider. The application will run in the same way.
**** Types of clouds
public cloud: The cloud services provided to you by a third party.
private cloud: When your company owns the services and the rest of your infrastructure, whether that's on-site or in a remote data center.
hybrid cloud: A mixture of both public and private clouds. Some services are run on a private cloud whereas others are run in public cloud. The secret to success is have a good integration.
multi-cloud: A mixture of public and/or private clouds across vendors.
** Managing Instances In The Cloud
*** spinning up VMs in the Cloud
All cloud provides provide you a console that lets you manage the services that you are using.
You'll be asked to choose a zone/region where your instance will be running. You also need to choose the characteristics of the virtual machine (such as CPU, ram, disk, etc..). You'll also need to choose which OS you'll be running.

You can use UI in the web app or use the CLI to manage your VMs. Using the CLI lets you create, modify, and delete VMs using scripts. This is great for automation. You can also automate the contents of the VM.

Reference images: Store the contents of a machine in a reusable format.
Templating: The process of capturing all of the systems configuration to let us create VMs in a repeatable way.

Reference images' format depend on the vendor but often, the result is a file called 'disk image'.

Disk image: A snapshot of a virtual machine's disk at a given point in time.

The disk image copy migh not be an exact copy of the original machine because some machine data changes (like the hostname and the IP address).

    This can be very useful if you need a fleet of 10k machines with identical software.
*** Creating a New VM using the GCP Web UI
console.cloud.google.com

To connect to web server on the VM, enable 'Allow HTTP traffic'.

**** Cool thingy
type '$ curl wttr.in' to check the weather in your current location.
https://github.com/chubin/wttr.in
*** Customizing VMs in GCP
You can change the services in the VM so that you can start a web application automatically.
    The default location of systemd service files on Linux is etc/systemd/system/

You can install puppet as a configuration management software.
**** Tip
'$ ps ax | grep hello'
lists all the running processes and searches for 'hello'
*** Templating a Customized VM
You can use the previous configured VM to create a template (this VM has puppet and a web server already configured).
    1. Stop the VM's disk on the control panel.
    2. Click on the instance's name.
    3. Create an image using 'create image'
    4. Configure the settings.
    5. Navigate to Instance templates and create a new one.
    6. Navigate to your customized images and select the one you want.
    7. Configure the instance settings.

You can now create instances using this image. You could use the UI on console.cloud.google.com or uyse the gcloud CLI.

If you need to create a lot of VMs using an image, using CLI can be faster. For example, creating 5 new VMs using a template called 'webserver-template' you can use the following code:

    'gcloud compute instances create --source-instance-template webserver-template ws1 ws2 ws3 ws4 ws5'

        This will create 5 new VMs called ws1 .. ws5.
*** Managing VMs in GCP
https://cloud.google.com/compute/docs/quickstart-linux

https://cloud.google.com/compute/docs/instances/create-vm-from-instance-template

https://cloud.google.com/sdk/docs
** Automating Cloud Deployments
*** Cloud Scale Deployments
Set up the services so that we can easily increase their capacity by adding more nodes to the pool. These nodes could be VMs, containers,  specific applications providing one service.

For a service with multiple instances, we can use a load balancer.

Load balancer: ensures that each node receives a balanced number of requests.
    When a request comes in, the load balancer picks a node to serve the response.

Load balancer strategies:
    Simplest load balancer is to give each node one request (called round robin)
    Another more complex strategy is to select the node closest to the requester or selecting the node with the least current load.
        This approach turns on nodes when there's more demand and shut some nodes down when the demand falls. This is call autoscaling.

Autoscaling: Allows the service to increase or reduce capacity as needed, while the service owner only pays for the cost of the machines that are in use at any given time.

        When the nodes shut down, their disk will also disappear and should be considered short-lived.

        If you need data persistence, you'll need to create separated storage resources to hold that data and connect that storage to the nodes. That is why cloud services are typically connected to a database, also in the cloud.

When accessing a website, the browser retrieves the IP address of the website. This IP specifies a specific computer: the entry point for the site (commonly, there are multiple entry sites for a single website, allowing the site to stay up even if one of them fails and allowing the user to connect to the closest entry point to reduce latency). For small apps, this entry point would be the web server. For large applications, where speed and availability, there will be a couple of layers between the entry point and the web server. The first layer is the web caching servers with load balancers to distribute requests between them. One popular application for this caching is called *varnish* but *nginx* also has this functionality. There are providers that do web caching as a service such as cloudflare and fastly. For cache of databases, there are services such as meemcached and redis.
*** What is orchestration?
Orchestration: The automated configuration and coordination of complex IT systems and services.

The configuration of the overall system needs to be automatically repeatable. CLI and UI of clouds systems usually do not allow this. They normally use application programming interface or API that lets us interact with the cloud infrastructure directly from our scripts.

In Cloud provider's APIs, they typically let you handle the configuration that you want directly from your scrips or programs without having to call a separate command. This gives us extra flexibility which is extra useful when dealing with complex systems.

We should set up monitoring and alerting rules so that we know if there is an issue before the end users are affected.
*** Cloud infrastructure as Code
Storing our infrastructure in code like format allows us to create repeatable infrastructure, and using version control allows us to keep a history of what we've done and rollback mistakes. Cloud infrastructure is similar but the storing might be a little bit different. Cloud provides have tools for managing resources as code (e.g. Amazon's Cloud Formation, Google's Cloud Deployment Manager, Microsoft's Azure Resource Manager, OpenStak's Heat Orchestration Templates).

Terrform uses its own Domain-specific language. Terraform knows how to interact with different cloud providers and automatic vendors. I.e. deploy terraform rules to deploy the service on one Cloud provider and use very similar rules to deploy to another Cloud provider.

    This way, you do not have to know every cloud providers APIs!

Puppet is for the specif VM (choose package and configuration) whereas Terraform is for defining things such as VM resources and where to deploy those VMs (specific APIs for each Cloud Provider).

Puppet can also be used (with plug-ins) for interacting with the cloud providers.

**** Dealing with nodes in the Cloud
Long-lived nodes: contents need to be periodically updated.
    E.g. company's internal mail server, or file sharing server.
Short-lived nodes: updates are done by deleting old instances and creating new ones.

Puppet can be used to update the long-lived nodes while running but also to update the short-lived nodes by starting them with a new configuration.
* Week 4: Managing Cloud Instances at Scale
** Building Software for the Cloud
*** Storing Data in The Cloud
Traditional storage technology : Block storage.
New storage technology: Object or Blob storage.

Block storage: This type of storage closely resembles the physical storage (physical storage). On virtual machines, the OS will create and manage a file system on top of the block storage.

This storage can be persistent or ephemeral.

Persistent storage: Used for instances that are long lived and need to keep data across reboots and upgrades.

Ephemeral storage: Used for instances that are only temporary and only need to keep local data while they are running.

If you are looking to share data across instances, look into shared file system solutions. This allows you to access data through network file system protocols like NFS and CIFS. This allows you to connect multiple instances or containers on the same file system without programming.

The block storage and shared file system is great for managing servers that need to access files.

However, if you need to store application data, you might want to take a  look into blob or object storage.

Object storage: Lets you place and retrieve objects in a storage bucket. (Blob: Binary Large OBject. Blobs, objects can be photos or cat videos.)

To interact with the storage bucket, you need to use an API or special utilities.

There are basically two database as a service flavors: SQL and NoSQL.

Cloud providers will charge you depending on the specs:
Throughput: The amount of data that you can read and write in a given amount of time.
IOPS (input/output operations per second): Measures how many reads and writes you can do in one second, no matter how much data you're accessing.
Latency: The amount of time it takes to complete a read or write operation.

Hot data: Data accessed frequently and stored in hot storage.
Cold data: Accessed infrequently and stored in cold storage.
*** Load Balancing
Once we set up replicated machines, we'll want to distribute the requests across instances.

Balancing technique: Round-Robin DNS.
    The URL is translate to an IP address using the DNS protocol (or Domain Name System). When using Round-Robin DNS, each client asking for the translation will get a group of IP addresses in different order. The client will try to access the first one, if this one fails, it will try the next one and so on... This is easy to configure (just make sure that the IPs of all machines in the pool are configured in your DNS server). However, there are a few downsides: you cannot choose which addresses get picked by the clients, DNS records are cached by the clients and other servers so if you need to change the IPs for the instances, you'll have to wait until all the DNS records that were cached by the clients expire.

There is a better way. Use a dedicated load balancer server which assigns the servers to the clients based on predefined rules. But you might want to have sticky sessions. By using a dedicated load balancer server, you can simply add/remove VMs and just let the load balancer server know. You could also use autoscaling so that the load balancer adds/removes VMs automatically.

Sticky sessions: All request from the same client always go to the same back-end server. (Only use it when really needed)

A lot of cloud providers have GeoIP or GeoDNS to assign servers to the clients based on their physical location so that the clients always get the nearest server.

Content Delivery Networks (CDNs): Make up a network of physical hosts that are geographically located as close to the end users as possible. (They cache data from the server whenever a user makes a request so that another user in that area does not have to wait as long).
*** Change Management
Usually, when something goes wrong is because something changed. That is why we need to make those changes in a controlled and safe way. To do so we need to make sure our changes are well tested (unit tests and integration tests). Ideally, the CI (continuous integration) runs even changes that are being reviewed so that the end user does not see any bugs. Once the change is committed, the CI will test the resulting code. CD (continuous deployment) lets you control the deployment with rules (e.g. only deploy the new builds when they pass all the tests).

Again, ideally, you should have a test environment separate from the production environment so that you push recent changes to the test environment (environment means everything needed to run the service, including machines, networks, the deployed code, the configuration management, the application configuration, and the customer data).

A/B testing: Some requests are served using one set of code (A) and other requests are served using a different set of code and configuration (B).
*** Understanding Limitations
Sometimes things fail, other times there are limitations such as quotas or limits (chosen at the Cloud provider).
Rate limits: Prevent one service from overloading the whole system. E.g. limit an expensive API call to 1 per seconds.
Utilization limits: Cap the total amount of a certain resource that you can provide. E.g. limit the amount of VMs you can deploy because they can be very expensive!
** Monitoring and Alerting
*** Getting Started with Monitoring
Response codes in the 500 range mean something went wrong with the server.
Response codes in the 400 range mean there was a client-side problem with the request.

E.g. you run a e-commerce site and you want to know how many purchases were completed and how many failed. To do this, you can store the data in a monitoring system such as AWS Cloudwatch, Google Stack Driver, Azure Metrics provided by cloud providers or others such as Prometheus, Data Dog, or Nagios can be used across vendors. You can create dashboards using these tools. Only store the metrics you care about! Space costs money!

Whitebox monitoring: Checks the behavior of the system from the inside. E.g. CPU usage, RAM usage, etc..
Blackbox monitoring: Checks the behavior of the system from the outside. E.g. making a request and see if the response matches the expected response.

We do not have to watch the dashboard the whole day because we can set up alerting rules.
*** Getting Alerts When Things Go Wrong
The easiest way to do this is to run a program periodically that checks the health of the system and emails the system administrator if something is wrong. In linux systems, we can use *cron* which is a tool to run scheduled jobs to run a python script.

When using a monitoring system, you can configure the system to periodically check the metrics and raise an alert if a rule applies. E.g. if the application is using 10gb of ram, or a very long queue.

You can divide alerts into two categories: page level alert (really urgent and critical alerts) and non-page alert (not so important and can be dealt with in an non-urgent fashion).
*** Service-Level Objectives
Service-Level Objectives (SLOs): Pre-established performance goals for a specific service.

SLOs need to be measurable meaning that there should be a metric to check if the performance meets the objectives or not. E.g. a service needs to be 99% available per year (i.e. means that 3 days per year of down time). E.g. 90% of the request should return within 5 seconds.

99% availability is called two-nines. 99.9% availability is called three-nines. 99.999% availability is called five-nines availability. And so on...

Service-Level Agreement (SLA): A commitment between a provider and a client.

If SLAs are unfulfilled, there might be serious consequences. Whereas SLOs are more of an internal metric.

Error budget: the amount of time you get before you unfulfill the SLOs (E.g. for 99.9% availability there is an error budget of 0.1% of down time .)
*** Basic Monitoring in GCP

Use the monitoring tool provided by Google Cloud, i.e. Stackdriver.

Generally speaking, you'll want to set up the alerting condition to go off whenever there is an event that fulfills a condition for 1 minute, 5 minutes, 10 minutes, etc.. You do not really need to concern yourself with spikes of a few seconds in length.
**** tip
in order to bring a program that is running in the background, you can use the command '$ fg'.
*** Troubleshooting and Debugging
**** What to Do When You Can't Be Physically There
If you have infrastructure-as-code and something goes wrong, you can deploy new instances. Also, if something does not boot, you could snapshot the troublesome machine and mount the image in a healthy machine to see what's wrong.

Try to isolate the faulty behavior. Set up a testing environment. Take a look at the logs. Some cloud providers gather all logs on a specific location.
**** Identify Where the Failure Is Coming From
It is hard to diagnose where the failure is coming from: from your application or from the cloud provider infrastructure. As always, try to isolate the failure. For example, change the geographical area of the service, change the same system in another machine type. Make sure you can rollback individual changes, this way, you can also identify the problem by removing every change until you find the culprit.

Containers: Packaged applications that are shipped together with their libraries and dependencies.

    Something cool about containers is that you can deploy the same container in your in-premises server, to your workstation, or to the cloud. Using containers you can check if the failure is on the cloud provider service by deploying the container somewhere else.

Ideally, you want to have many containers, each dedicated for a specific job of the application and have very good logs for all the containers.
**** Recovering from Failure
To have a reliable service, it is important to have a way to be get it up and running as quickly as possible.

If you store any kind of data, it is important to regularly back up that data and to check that the back up service works as intended by performing restores.

If you follow infrastructure-as-code practices, you already have a backup but getting things up and running might take a while. You might want to consider having secondary instances of the services (so that, in case of a failure in the primary instances, you simple tell the DNS server or the load balancer to direct the traffic somewhere else).

Consider running drills so that you can try the documentation to get the service back up and running.

So consieder the following:
- Having multiple points of redundancy.
- Having a well-documented disaster recovery plan.
- Having automatic backups
